{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing : Classic to Deep Methods for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-Of-Word and TF-IDF:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "\n",
    "Recurrent Neural Networks (RNNs):\n",
    "\n",
    "https://medium.com/towards-data-science/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
    "\n",
    "Long Short Term Memory networks (LSTMs):\n",
    "\n",
    "https://medium.com/towards-data-science/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Word embeddings:\n",
    "\n",
    "http://jalammar.github.io/illustrated-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#TOFILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to tackle the sentiment analysis problem, a *text classification* problem. The idea is pretty simple : we want to automatically predict whether a text expresses positive or negative sentiments. To do so we will use the IMDB dataset, that contains 50000 movie reviews from the www.imdb.com website, and their corresponding sentiment : positive or negative. It is thus a binary classification problem, where we want to predict a binary target $y \\in \\{0,1\\}$. We will go through different ways of encoding a text in a vectorial form $x \\in \\mathbb{R}^d$, as well as different classification models, from classic ways to modern deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore a bit the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw this movie last night after waiting ages...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This was, so far, the worst movie I have seen ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In a time of magic, barbarians and demons abou...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had high expectations of this movie (the tit...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a film of immense appeal to a relative...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>An hilariously accurate caricature of trying t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I watch most movies that Nick Mancuso is in be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This is one of those films that's more interes...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A wonderful and gritty war film that focuses o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Man, some of you people have got to chill. Thi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  I saw this movie last night after waiting ages...  positive\n",
       "1  This was, so far, the worst movie I have seen ...  negative\n",
       "2  In a time of magic, barbarians and demons abou...  positive\n",
       "3  I had high expectations of this movie (the tit...  negative\n",
       "4  This is a film of immense appeal to a relative...  negative\n",
       "5  An hilariously accurate caricature of trying t...  positive\n",
       "6  I watch most movies that Nick Mancuso is in be...  positive\n",
       "7  This is one of those films that's more interes...  negative\n",
       "8  A wonderful and gritty war film that focuses o...  positive\n",
       "9  Man, some of you people have got to chill. Thi...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and print the dataset\n",
    "imdb_dataset_original=pd.read_csv('../data/IMDB Dataset.csv')\n",
    "imdb_dataset = imdb_dataset_original.copy()\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw this movie last night after waiting ages and ages for it to be released here in Canada (still only in limited release). It was worth the wait and then some. I am a very avid reader of Margaret Laurence and was excited to see that this novel was being turned into a film. I actually ended up liking the movie better than the novel. I liked that the character of Bram Shipley was a bit less harsh, and that there seemed to be more of a love story between Hagar and Bram, which made the scenes at the end of Bram's life that much more moving. The loss seemed stronger. Hagar was not any more likable on film than in the book, but Ellen Burstyn was a genius in this role. She WAS Hagar through and through. Christine Horne was brilliant and has many more great things ahead I am sure. Her scenes with Cole Hauser were electrifying. I could go on and on, overall a 9 * out of 10. Fantastic and can't wait for it to come out on DVD, a must own for my collection!\n"
     ]
    }
   ],
   "source": [
    "#Print first review:\n",
    "print(imdb_dataset[\"review\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the two classes size\n",
    "imdb_dataset['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "As you can see the text is quite messy, and before encoding our text into features, we are going to go through different preprocessing steps in order to clean it:\n",
    "* Removing the HTML tags.\n",
    "* Removing other special characters : this means all non alphanumeric characters, including punctuation.\n",
    "* Lowercase the text.\n",
    "* Tokenization : split a text as a list of words now called tokens.\n",
    "* Stemming : removing all the suffixes from conjugation, plural, ... In order to bring a word back to its root form. For example.\n",
    "* Removing stopwords : words like 'to', 'a', 'the', ... are called stopwords, we remove them as they are too frequent words and generally just add noise.\n",
    "\n",
    "Fill the following functions to perform each of these steps. You are free to use the libraries of your choice to do so. Try to not reinvent the wheel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow. I don't even really remember that much about this movie, except that it stunk.<br /><br />The plot's basically; a girl's parents neglect her, so this sicko PokeMon pretends to be her dad. Am I the only one disturbed by that? Then, this weirdo PokeMon kidnaps Ash's mom to pretend to be the girl's. I don't care if he was trying to make the girl happy, that's just gross.<br /><br />There was no real plot. The girl was just a whiny brat who wanted things her own way. She played with Unowns, was the \"daughter\" of Entei and apparently could grow and shrink in age on a whim with the help of her \"dad\".<br /><br />That's pretty much all I can remember, but I think you can take it as a hint, and not see it. (Or if you do see it, don't expect much.) 1 out of 10.<br /><br />Seriously. If you want a PokeMon movie, rent \"PokeMon; the First Movie\".\n"
     ]
    }
   ],
   "source": [
    "### HTML encoding as the exemple \n",
    "print(imdb_dataset[\"review\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BeautifulSoup is a function from bs4 library which remove HMTL tags\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## see https://www.geeksforgeeks.org/python/remove-all-style-scripts-and-html-tags-using-beautifulsoup/\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to clean from html tags\n",
    "    Output: str : The same string with html tags removed\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text  ## securit√© si ce n'est pas une string\n",
    "    \n",
    "    # texte HTLM\n",
    "    soup = BeautifulSoup(text,\"html.parser\")  \n",
    "    for data in soup(['style', 'script']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "\n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "    #return clean_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw this movie last night after waiting ages...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This was, so far, the worst movie I have seen ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In a time of magic, barbarians and demons abou...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had high expectations of this movie (the tit...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a film of immense appeal to a relative...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>An hilariously accurate caricature of trying t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I watch most movies that Nick Mancuso is in be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This is one of those films that's more interes...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A wonderful and gritty war film that focuses o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Man, some of you people have got to chill. Thi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  I saw this movie last night after waiting ages...  positive\n",
       "1  This was, so far, the worst movie I have seen ...  negative\n",
       "2  In a time of magic, barbarians and demons abou...  positive\n",
       "3  I had high expectations of this movie (the tit...  negative\n",
       "4  This is a film of immense appeal to a relative...  negative\n",
       "5  An hilariously accurate caricature of trying t...  positive\n",
       "6  I watch most movies that Nick Mancuso is in be...  positive\n",
       "7  This is one of those films that's more interes...  negative\n",
       "8  A wonderful and gritty war film that focuses o...  positive\n",
       "9  Man, some of you people have got to chill. Thi...  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(remove_html_tags)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow. I don\\'t even really remember that much about this movie, except that it stunk. The plot\\'s basically; a girl\\'s parents neglect her, so this sicko PokeMon pretends to be her dad. Am I the only one disturbed by that? Then, this weirdo PokeMon kidnaps Ash\\'s mom to pretend to be the girl\\'s. I don\\'t care if he was trying to make the girl happy, that\\'s just gross. There was no real plot. The girl was just a whiny brat who wanted things her own way. She played with Unowns, was the \"daughter\" of Entei and apparently could grow and shrink in age on a whim with the help of her \"dad\". That\\'s pretty much all I can remember, but I think you can take it as a hint, and not see it. (Or if you do see it, don\\'t expect much.) 1 out of 10. Seriously. If you want a PokeMon movie, rent \"PokeMon; the First Movie\".'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utilisation de la focntion translate() du package string (pas besoin de l'installer)\n",
    "# https://www.geeksforgeeks.org/python/python-removing-unwanted-characters-from-string/\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to clean from non alphanumeric characters\n",
    "    Output: str : The same strings without non alphanumeric characters\n",
    "    \"\"\"\n",
    "    res= text.translate(str.maketrans('','',string.punctuation))\n",
    "    return(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I saw this movie last night after waiting ages...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This was so far the worst movie I have seen in...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In a time of magic barbarians and demons aboun...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had high expectations of this movie the titl...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a film of immense appeal to a relative...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>An hilariously accurate caricature of trying t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I watch most movies that Nick Mancuso is in be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This is one of those films thats more interest...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A wonderful and gritty war film that focuses o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Man some of you people have got to chill This ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  I saw this movie last night after waiting ages...  positive\n",
       "1  This was so far the worst movie I have seen in...  negative\n",
       "2  In a time of magic barbarians and demons aboun...  positive\n",
       "3  I had high expectations of this movie the titl...  negative\n",
       "4  This is a film of immense appeal to a relative...  negative\n",
       "5  An hilariously accurate caricature of trying t...  positive\n",
       "6  I watch most movies that Nick Mancuso is in be...  positive\n",
       "7  This is one of those films thats more interest...  negative\n",
       "8  A wonderful and gritty war film that focuses o...  positive\n",
       "9  Man some of you people have got to chill This ...  positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(remove_special_characters)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow I dont even really remember that much about this movie except that it stunk The plots basically a girls parents neglect her so this sicko PokeMon pretends to be her dad Am I the only one disturbed by that Then this weirdo PokeMon kidnaps Ashs mom to pretend to be the girls I dont care if he was trying to make the girl happy thats just gross There was no real plot The girl was just a whiny brat who wanted things her own way She played with Unowns was the daughter of Entei and apparently could grow and shrink in age on a whim with the help of her dad Thats pretty much all I can remember but I think you can take it as a hint and not see it Or if you do see it dont expect much 1 out of 10 Seriously If you want a PokeMon movie rent PokeMon the First Movie'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lowercase with str.lower() https://www.programiz.com/python-programming/methods/string/lower\n",
    "\n",
    "def lowercase_text(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to lowercase\n",
    "    Output: str : The same string lowercased\n",
    "    \"\"\"\n",
    "    res=text.lower()\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i saw this movie last night after waiting ages...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this was so far the worst movie i have seen in...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in a time of magic barbarians and demons aboun...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i had high expectations of this movie the titl...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is a film of immense appeal to a relative...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>an hilariously accurate caricature of trying t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i watch most movies that nick mancuso is in be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this is one of those films thats more interest...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a wonderful and gritty war film that focuses o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>man some of you people have got to chill this ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  i saw this movie last night after waiting ages...  positive\n",
       "1  this was so far the worst movie i have seen in...  negative\n",
       "2  in a time of magic barbarians and demons aboun...  positive\n",
       "3  i had high expectations of this movie the titl...  negative\n",
       "4  this is a film of immense appeal to a relative...  negative\n",
       "5  an hilariously accurate caricature of trying t...  positive\n",
       "6  i watch most movies that nick mancuso is in be...  positive\n",
       "7  this is one of those films thats more interest...  negative\n",
       "8  a wonderful and gritty war film that focuses o...  positive\n",
       "9  man some of you people have got to chill this ...  positive"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(lowercase_text)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different methods de tokenisation https://www.geeksforgeeks.org/nlp/5-simple-ways-to-tokenize-text-in-python/\n",
    "# str.split() method is a very efficient method on large datasets, but only works with tabular text data, no punctuatiob handling\n",
    "\n",
    "def tokenize_words(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to tokenize\n",
    "    Output: list of str : A list of the tokens splitted from the input string\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = text.split()\n",
    "\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, saw, this, movie, last, night, after, wait...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[this, was, so, far, the, worst, movie, i, hav...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[in, a, time, of, magic, barbarians, and, demo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[i, had, high, expectations, of, this, movie, ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[this, is, a, film, of, immense, appeal, to, a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[an, hilariously, accurate, caricature, of, tr...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[i, watch, most, movies, that, nick, mancuso, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[this, is, one, of, those, films, thats, more,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[a, wonderful, and, gritty, war, film, that, f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[man, some, of, you, people, have, got, to, ch...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  [i, saw, this, movie, last, night, after, wait...  positive\n",
       "1  [this, was, so, far, the, worst, movie, i, hav...  negative\n",
       "2  [in, a, time, of, magic, barbarians, and, demo...  positive\n",
       "3  [i, had, high, expectations, of, this, movie, ...  negative\n",
       "4  [this, is, a, film, of, immense, appeal, to, a...  negative\n",
       "5  [an, hilariously, accurate, caricature, of, tr...  positive\n",
       "6  [i, watch, most, movies, that, nick, mancuso, ...  positive\n",
       "7  [this, is, one, of, those, films, thats, more,...  negative\n",
       "8  [a, wonderful, and, gritty, war, film, that, f...  positive\n",
       "9  [man, some, of, you, people, have, got, to, ch...  positive"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(tokenize_words)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/marie.vanacker@Digital-\n",
      "[nltk_data]     Grenoble.local/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stopwords https://vectorize.io/blog/removing-nltk-stopwords-with-python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def remove_stopwords(token_list):\n",
    "    \"\"\"\n",
    "    Input: list of str : A list of tokens\n",
    "    Output: list of str : The new list with removed stopwords tokens\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #they are present in stop_words or not\n",
    "    filtered_sentence = [w for w in token_list if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in token_list:\n",
    "\t    if w not in stop_words:\n",
    "\t\t    filtered_sentence.append(w)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[saw, movie, last, night, waiting, ages, ages,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[far, worst, movie, seen, entire, life, seen, ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[time, magic, barbarians, demons, abound, diab...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[high, expectations, movie, title, translated,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[film, immense, appeal, relatively, welldefine...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[hilariously, accurate, caricature, trying, se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[watch, movies, nick, mancuso, frankly, love, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[one, films, thats, interesting, watch, academ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[wonderful, gritty, war, film, focuses, inner,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[man, people, got, chill, movie, artistic, gen...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  [saw, movie, last, night, waiting, ages, ages,...  positive\n",
       "1  [far, worst, movie, seen, entire, life, seen, ...  negative\n",
       "2  [time, magic, barbarians, demons, abound, diab...  positive\n",
       "3  [high, expectations, movie, title, translated,...  negative\n",
       "4  [film, immense, appeal, relatively, welldefine...  negative\n",
       "5  [hilariously, accurate, caricature, trying, se...  positive\n",
       "6  [watch, movies, nick, mancuso, frankly, love, ...  positive\n",
       "7  [one, films, thats, interesting, watch, academ...  negative\n",
       "8  [wonderful, gritty, war, film, focuses, inner,...  positive\n",
       "9  [man, people, got, chill, movie, artistic, gen...  positive"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(remove_stopwords)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.geeksforgeeks.org/machine-learning/python-stemming-words-with-nltk/ => cf Thibault github pour enlever reduce\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_words(token_list):\n",
    "    \"\"\"\n",
    "    Input: list of str : A list of tokens to stem\n",
    "    Output: list of str : The list of stemmed tokens\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    stemmed_list = [ps.stem(word) for word in token_list]\n",
    "    return stemmed_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[saw, movi, last, night, wait, age, age, relea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[far, worst, movi, seen, entir, life, seen, re...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[time, magic, barbarian, demon, abound, diabol...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[high, expect, movi, titl, translat, get, rid,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[film, immens, appeal, rel, welldefin, group, ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[hilari, accur, caricatur, tri, sell, script, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[watch, movi, nick, mancuso, frankli, love, gu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[one, film, that, interest, watch, academ, per...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[wonder, gritti, war, film, focus, inner, torm...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[man, peopl, got, chill, movi, artist, geniu, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  [saw, movi, last, night, wait, age, age, relea...  positive\n",
       "1  [far, worst, movi, seen, entir, life, seen, re...  negative\n",
       "2  [time, magic, barbarian, demon, abound, diabol...  positive\n",
       "3  [high, expect, movi, titl, translat, get, rid,...  negative\n",
       "4  [film, immens, appeal, rel, welldefin, group, ...  negative\n",
       "5  [hilari, accur, caricatur, tri, sell, script, ...  positive\n",
       "6  [watch, movi, nick, mancuso, frankli, love, gu...  positive\n",
       "7  [one, film, that, interest, watch, academ, per...  negative\n",
       "8  [wonder, gritti, war, film, focus, inner, torm...  positive\n",
       "9  [man, peopl, got, chill, movi, artist, geniu, ...  positive"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(stem_words)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join all that together and apply it to our dataset. The following function simply chains all the preprocessing steps you just implemented. \n",
    "\n",
    "It adds the `list_output` flag, if False it will reconcatenate all the preprocessed tokens into a single string (with spaces between tokens), if True it will keep each sentence as a list of tokens. Depending on the libraries you will use for the next steps, it can be useful to have one or the other representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_dataset(dataset, text_col_name = 'review', html_tags = True,\n",
    "                           special_chars = True, lowercase = True , stemming = True , \n",
    "                           stopwords = True, list_output = False ):\n",
    "    \"\"\"\n",
    "    Apply the choosen preprocessing steps to a corpus of texts and return the \n",
    "    preprocessed corpus. The list_output flag allows to return either a list\n",
    "    of token, or a rejoined string with spaces between the preprocessed tokens.\n",
    "    \"\"\"\n",
    "    def rejoin_text(token_list):\n",
    "        return ' '.join(token_list)\n",
    "    \n",
    "    \n",
    "    output = dataset.copy()\n",
    "    \n",
    "    if html_tags : \n",
    "        output[text_col_name] = output[text_col_name].apply(remove_html_tags)\n",
    "        \n",
    "    if special_chars :\n",
    "        output[text_col_name] = output[text_col_name].apply(remove_special_characters)\n",
    "        \n",
    "    if lowercase :\n",
    "        output[text_col_name] = output[text_col_name].apply(lowercase_text)\n",
    "    \n",
    "    #Tokenization for next steps:\n",
    "    output[text_col_name] = output[text_col_name].apply(tokenize_words)\n",
    "    \n",
    "    if stopwords :\n",
    "        output[text_col_name] = output[text_col_name].apply(remove_stopwords)\n",
    "        \n",
    "    if stemming :\n",
    "        output[text_col_name] = output[text_col_name].apply(stem_words)\n",
    "        \n",
    "    if not list_output :\n",
    "        output[text_col_name] = output[text_col_name].apply(rejoin_text)\n",
    "        \n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_clean_dataset = normalize_text_dataset(imdb_dataset_original, html_tags = True,\n",
    "                           special_chars = True, lowercase = True , stemming = True , \n",
    "                           stopwords = True, list_output = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saw movi last night wait age age releas canada...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>far worst movi seen entir life seen realli bad...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>time magic barbarian demon abound diabol tyran...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high expect movi titl translat get rid other c...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>film immens appeal rel welldefin group part we...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hilari accur caricatur tri sell script documen...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>watch movi nick mancuso frankli love guy even ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>one film that interest watch academ perspect e...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wonder gritti war film focus inner torment bli...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>man peopl got chill movi artist geniu instead ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  saw movi last night wait age age releas canada...  positive\n",
       "1  far worst movi seen entir life seen realli bad...  negative\n",
       "2  time magic barbarian demon abound diabol tyran...  positive\n",
       "3  high expect movi titl translat get rid other c...  negative\n",
       "4  film immens appeal rel welldefin group part we...  negative\n",
       "5  hilari accur caricatur tri sell script documen...  positive\n",
       "6  watch movi nick mancuso frankli love guy even ...  positive\n",
       "7  one film that interest watch academ perspect e...  negative\n",
       "8  wonder gritti war film focus inner torment bli...  positive\n",
       "9  man peopl got chill movi artist geniu instead ...  positive"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_clean_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Text classification with Bag-Of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Now we have cleaned the reviews of our dataset, how do we represent them as vectors in order to classify it ? \n",
    "One classic way to achieve that is the Bag-Of-Words (BOW) approach. To encode a text in a bag of word, we first need to know all the different words $w$ that appear in all our reviews, called the vocabulary : $w \\in \\mathcal{V}$. For each word $w$ we attribute an index $idx(w) = i$ with $i \\in \\{0, |\\mathcal{V}|-1\\}$, and represent a review $r$ as a vector of the size of the vocabulary $x_r \\in \\mathbb{R}^{|\\mathcal{V}|}$. To encode a review we are simply going to count how many time each word appears and assign it at its corresponding index in the bag-of-words vector : $x_{r,i} = count(w,r)$, where i = idx(w). \n",
    "\n",
    "This means that we completely disregard the words order, and simply take into account the number of times each word appears in each review to represent them. There are many variations of this concept, TF-IDF (term frequency-inverse document frequency) for example, gives more weight to uncommon words. Read more about BOW and TF-IDF there:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "\n",
    "Let's start with bag-of-words. In general we don't consider the whole vocabulary but only some of the most frequent words in order to reduce the dimensionality and avoid noise from rare words. Here we will only consider the 10000 most frequent words of the training set, meaning the words that are only in the test set will be ignored. Thus we have : $x_r \\in \\mathbb{R^{10000}}$.\n",
    "\n",
    "Encode all the reviews as bag-of-words, and train and evaluate a logistic regression model on the following train test splits. As we have seen previously, if we wanted to investigate this model we should also grid search for hyperparameters by doing a cross-validation with validation sets, etc. However this is not the goal today, so we'll simply go for a train/test split for this experiment. Concerning the evaluation metrics, in this case we care equally about correctly predicting the positives and the negatives, and we have a balanced dataset, thus we can simply use accuracy this time.\n",
    "\n",
    "Once again, don't do everything from scratch and try to find libraries that propose implementations of these concepts !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words +> Repr√©sente un document comme un sac de mots, sans ordre ni contexte.\n",
    "On compte combien de fois chaque mot appara√Æt.      \n",
    "\n",
    "‚ÄúI loved the movie‚Äù ‚Üí {‚Äúi‚Äù:1, ‚Äúloved‚Äù:1, ‚Äúthe‚Äù:1, ‚Äúmovie‚Äù:1}        \n",
    "‚ÄúThe movie was not good‚Äù ‚Üí {‚Äúthe‚Äù:1, ‚Äúmovie‚Äù:1, ‚Äúwas‚Äù:1, ‚Äúnot‚Äù:1, ‚Äúgood‚Äù:1}         \n",
    "\n",
    "Pas de nuance :     \n",
    "‚Äúgood‚Äù = positif            \n",
    "‚Äúnot good‚Äù ‚Üí BOW ne voit pas le ‚Äúnot‚Äù coll√© au mot      \n",
    "\n",
    "‚Üí d‚Äôo√π les limites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "max_vocab_size = 10000 \n",
    "\n",
    "#Train/test split:\n",
    "\n",
    "lb=LabelBinarizer()\n",
    "sentiment_labels=lb.fit_transform(imdb_clean_dataset['sentiment'])\n",
    "\n",
    "\n",
    "train_reviews = imdb_clean_dataset.review[:25000]\n",
    "test_reviews = imdb_clean_dataset.review[25000:]\n",
    "\n",
    "#on vectorize\n",
    "train_sentiments = sentiment_labels[:25000]\n",
    "test_sentiments = sentiment_labels[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_reviews : 0    saw movi last night wait age age releas canada...\n",
      "1    far worst movi seen entir life seen realli bad...\n",
      "2    time magic barbarian demon abound diabol tyran...\n",
      "3    high expect movi titl translat get rid other c...\n",
      "4    film immens appeal rel welldefin group part we...\n",
      "Name: review, dtype: object\n",
      "test_reviews: 25000    intent purpos teen devian might seem like anot...\n",
      "25001    black dragon 1942 william nigh bela lugosi joa...\n",
      "25002    retic see flick read extern review user commen...\n",
      "25003    anoth exampl womeninprison genr exactli genr k...\n",
      "25004    wealthi hors rancher bueno air longstand notra...\n",
      "Name: review, dtype: object\n",
      "train_sentiments: [[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "test_sentiments: [[1]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"train_reviews :\",train_reviews.head(5))\n",
    "print(\"test_reviews:\",test_reviews.head(5))\n",
    "\n",
    "print(\"train_sentiments:\",train_sentiments)\n",
    "print(\"test_sentiments:\",test_sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words avec fit_transfom() sur le train et transform sur le test\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=max_vocab_size)\n",
    "\n",
    "# fit_transform sur le TRAIN uniquement\n",
    "X_train = vectorizer.fit_transform(train_reviews)\n",
    "\n",
    "# puis transform sur le TEST\n",
    "X_test = vectorizer.transform(test_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marie.vanacker@Digital-Grenoble.local/anaconda3/envs/keras_gpu/lib/python3.12/site-packages/sklearn/utils/validation.py:1406: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, train_sentiments)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(test_sentiments, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words + Logestic Rregression accuracy : 85.03%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bag-of-words + Logestic Rregression accuracy : {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get about 85% accuracy, pretty good for such a simple model. Now let's do the same with a tf-idf encoding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TF-IDF (Unigrams)** # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF => am√©liore Bag-of-Words en pond√©rant les mots selon leur importance.\n",
    "\n",
    "Un mot est important si :       \n",
    "    - il est fr√©quent dans CE document (TF)     \n",
    "    - il est rare dans le corpus entier (IDF)           \n",
    "\n",
    "Exemple intuitif :\n",
    "‚Äúmovie‚Äù ‚Üí tr√®s fr√©quent partout ‚Üí poids faible          \n",
    "‚Äúmasterpiece‚Äù, ‚Äúboring‚Äù, ‚Äúterrible acting‚Äù ‚Üí plus informatifs ‚Üí poids fort          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marie.vanacker@Digital-Grenoble.local/anaconda3/envs/keras_gpu/lib/python3.12/site-packages/sklearn/utils/validation.py:1406: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "max_vocab_size = 10000\n",
    "\n",
    "# On r√©utilise train_reviews / test_reviews d√©finis avant\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=max_vocab_size # m√™me fix que pour CountVectorizer\n",
    ")\n",
    "\n",
    "# TF-IDF sur le TRAIN\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_reviews)\n",
    "\n",
    "# TF-IDF sur le TEST (m√™me vocabulaire)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_reviews)\n",
    "\n",
    "# M√™me mod√®le: logistic regression\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train_tfidf, train_sentiments)\n",
    "\n",
    "# Pr√©diction + accuracy\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "acc_tfidf = accuracy_score(test_sentiments, y_pred_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + LogisticRegression accuracy: 88.04%\n"
     ]
    }
   ],
   "source": [
    "print(f\"TF-IDF + LogisticRegression accuracy: {acc_tfidf*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "And you should get about 88% accuracy this time. Other classic but more sophisticated features include N-grams, part-of-speech tagging and syntax trees, you can read more about these there:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/07/part-of-speechpos-tagging-dependency-parsing-and-constituency-parsing-in-nlp/\n",
    "\n",
    "But we will stop there for the classic approaches and go to deep learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TF-IDF avec N-grams (Unigrams + Bigrams)** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...unless you are ahead of time, in this case learn about Bag of N-grams by yourself, and try them out :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C‚Äôest quoi un N-gram ?\n",
    "\n",
    "Si on prend la phrase :\n",
    "\n",
    "\"this movie is very good\"\n",
    "\n",
    "Unigrams (1-grams) :\n",
    "[\"this\", \"movie\", \"is\", \"very\", \"good\"]\n",
    "\n",
    "Bigrams (2-grams) :\n",
    "[\"this movie\", \"movie is\", \"is very\", \"very good\"]\n",
    "\n",
    "Trigrams (3-grams) :\n",
    "[\"this movie is\", \"movie is very\", \"is very good\"]\n",
    "\n",
    "Un bag of N-grams, c‚Äôest exactement comme Bag-of-Words, mais o√π les ‚Äúmots‚Äù peuvent √™tre des s√©quences de 2, 3 mots, etc.\n",
    "\n",
    "Int√©r√™t :           \n",
    "- les unigrams voient les mots\n",
    "- les bigrams capturent des motifs comme :              \n",
    "    \"not good\" (n√©gatif)            \n",
    "    \"very good\" (positif)                   \n",
    "    \"no plot\", \"poor acting\" ‚Ä¶\n",
    "\n",
    "Ce que BOW ou TF-IDF unigram only ratent souvent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marie.vanacker@Digital-Grenoble.local/anaconda3/envs/keras_gpu/lib/python3.12/site-packages/sklearn/utils/validation.py:1406: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "max_vocab_size = 20000  # un peu plus grand car beaucoup plus de n-grams possibles\n",
    "\n",
    "tfidf_vectorizer_ngrams = TfidfVectorizer(\n",
    "    max_features=max_vocab_size,\n",
    "    ngram_range=(1, 2)   \n",
    ")\n",
    "\n",
    "# Apprentissage du vocabulaire + TF-IDF sur le TRAIN\n",
    "X_train_ngrams = tfidf_vectorizer_ngrams.fit_transform(train_reviews)\n",
    "\n",
    "# TF-IDF sur le TEST avec le m√™me vocabulaire\n",
    "X_test_ngrams = tfidf_vectorizer_ngrams.transform(test_reviews)\n",
    "\n",
    "# Mod√®le : logistic regression comme avant\n",
    "clf_ngrams = LogisticRegression(max_iter=1000)\n",
    "clf_ngrams.fit(X_train_ngrams, train_sentiments)\n",
    "\n",
    "# √âvaluation\n",
    "y_pred_ngrams = clf_ngrams.predict(X_test_ngrams)\n",
    "acc_ngrams = accuracy_score(test_sentiments, y_pred_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-IDF (1-2 grams) + Logistic Regression accuracy: 88.77%\n"
     ]
    }
   ],
   "source": [
    "print(f\"F-IDF (1-2 grams) + Logistic Regression accuracy: {acc_ngrams*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive n-grams:\n",
      "['still' 'highli' 'superb' 'fantast' 'one best' 'brilliant' 'today'\n",
      " 'definit' '710' 'fun' 'well' 'amaz' 'beauti' 'favorit' 'best' 'love'\n",
      " 'enjoy' 'perfect' 'excel' 'great']\n",
      "\n",
      "Top negative n-grams:\n",
      "['worst' 'bad' 'wast' 'aw' 'bore' 'poor' 'disappoint' 'noth' 'terribl'\n",
      " 'wors' 'fail' 'dull' 'horribl' 'poorli' 'unfortun' 'stupid' 'lack'\n",
      " 'suppos' 'ridicul' 'save']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = np.array(tfidf_vectorizer_ngrams.get_feature_names_out())\n",
    "coef = clf_ngrams.coef_[0]\n",
    "\n",
    "# Top 20 n-grams les plus \"positifs\"\n",
    "top_pos_idx = np.argsort(coef)[-20:]\n",
    "print(\"Top positive n-grams:\")\n",
    "print(feature_names[top_pos_idx])\n",
    "\n",
    "# Top 20 n-grams les plus \"n√©gatifs\"\n",
    "top_neg_idx = np.argsort(coef)[:20]\n",
    "print(\"\\nTop negative n-grams:\")\n",
    "print(feature_names[top_neg_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Save the data* # \n",
    "Preprocessing for the deep learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_deep_clean_dataset = normalize_text_dataset(imdb_dataset_original, html_tags = True,\n",
    "                           special_chars = False, lowercase = True, stemming = False, \n",
    "                           stopwords = False, list_output = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i saw this movie last night after waiting ages...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this was, so far, the worst movie i have seen ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in a time of magic, barbarians and demons abou...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i had high expectations of this movie (the tit...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is a film of immense appeal to a relative...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>an hilariously accurate caricature of trying t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i watch most movies that nick mancuso is in be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this is one of those films that's more interes...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a wonderful and gritty war film that focuses o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>man, some of you people have got to chill. thi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  i saw this movie last night after waiting ages...  positive\n",
       "1  this was, so far, the worst movie i have seen ...  negative\n",
       "2  in a time of magic, barbarians and demons abou...  positive\n",
       "3  i had high expectations of this movie (the tit...  negative\n",
       "4  this is a film of immense appeal to a relative...  negative\n",
       "5  an hilariously accurate caricature of trying t...  positive\n",
       "6  i watch most movies that nick mancuso is in be...  positive\n",
       "7  this is one of those films that's more interes...  negative\n",
       "8  a wonderful and gritty war film that focuses o...  positive\n",
       "9  man, some of you people have got to chill. thi...  positive"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_deep_clean_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deep_clean = imdb_deep_clean_dataset.iloc[:20000]\n",
    "valid_deep_clean = imdb_deep_clean_dataset.iloc[20000:25000]\n",
    "test_deep_clean = imdb_deep_clean_dataset.iloc[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '../data/imdb_clean/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deep_clean.to_csv(outdir + 'train.csv', index = False)\n",
    "valid_deep_clean.to_csv(outdir + 'valid.csv', index = False)\n",
    "test_deep_clean.to_csv(outdir + 'test.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
